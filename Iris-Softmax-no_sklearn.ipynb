{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2]])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,(2,3)]\n",
    "y = iris.target.astype(int)\n",
    "\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 1.4, 0.2],\n",
       "       [1. , 1.4, 0.2],\n",
       "       [1. , 1.3, 0.2],\n",
       "       [1. , 1.5, 0.2],\n",
       "       [1. , 1.4, 0.2]])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bias = np.c_[np.ones((len(X),1)),X]\n",
    "X_bias[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X,y,shuffle=True,test_ratio=0.2,val_ratio=0.2):\n",
    "    size = len(X)\n",
    "    if shuffle:\n",
    "        indexes = np.random.permutation(len(X))\n",
    "    else:\n",
    "        indexes = range(len(X))\n",
    "\n",
    "    test_size = int(size*test_ratio)\n",
    "    val_size = int(test_size*val_ratio)\n",
    "    train_size = size-test_size-val_size\n",
    "\n",
    "    X_train = X[indexes[:train_size]]\n",
    "    y_train = y[indexes[:train_size]]\n",
    "    X_val = X[indexes[train_size:-test_size]]\n",
    "    y_val = y[indexes[train_size:-test_size]]\n",
    "    X_test = X[indexes[-test_size:]]\n",
    "    y_test = y[indexes[-test_size:]]\n",
    "\n",
    "    return X_train,y_train,X_val,y_val,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 1.9, 0.2],\n",
       "       [1. , 3.6, 1.3],\n",
       "       [1. , 5.9, 2.1],\n",
       "       [1. , 6.7, 2.2],\n",
       "       [1. , 6.7, 2. ]])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,y_train,X_val,y_val,X_test,y_test = train_test_val_split(X_bias,y)\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hot_encode(y):\n",
    "    num_classes = y.max()+1\n",
    "    m = len(y)\n",
    "    y_hot_encode = np.zeros((m,num_classes))\n",
    "    y_hot_encode[np.arange(m), y] = 1 \n",
    "    return y_hot_encode\n",
    "\n",
    "y_train_encode = hot_encode(y_train)\n",
    "y_test_encode = hot_encode(y_test)\n",
    "y_val_encode = hot_encode(y_val)\n",
    "y_train_encode[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X,Theta):\n",
    "    return X.dot(Theta)\n",
    "\n",
    "def softmax_function(scores):\n",
    "    dividend = np.exp(scores)\n",
    "    divisor = np.sum(dividend, axis=1, keepdims=True)\n",
    "    return dividend/divisor\n",
    "\n",
    "def cross_entropy_cost_function(y,probabilities):\n",
    "    return -np.mean(np.sum(y_train_encode*np.log(probabilities),axis=1))\n",
    "\n",
    "def cross_entropy_gradient(X,y,probabilities):\n",
    "    m = len(X)\n",
    "    return (1/m)*X.T.dot(probabilities-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0      Cost function: 1.7809351823164337\n",
      "Iteration # 500      Cost function: 0.7853082500658067\n",
      "Iteration # 1000      Cost function: 0.662175381833871\n",
      "Iteration # 1500      Cost function: 0.5866044593945988\n",
      "Iteration # 2000      Cost function: 0.535453912319336\n",
      "Iteration # 2500      Cost function: 0.49804949859141173\n",
      "Iteration # 3000      Cost function: 0.46909122544766735\n",
      "Iteration # 3500      Cost function: 0.4457124769766781\n",
      "Iteration # 4000      Cost function: 0.4262389179734997\n",
      "Iteration # 4500      Cost function: 0.40962717736569243\n",
      "Iteration # 5000      Cost function: 0.3951912238521942\n"
     ]
    }
   ],
   "source": [
    "num_features = X_bias.shape[1] \n",
    "num_classes = len(np.unique(y))\n",
    "Theta = np.random.rand(num_features,num_classes)\n",
    "epsilon = 1e-6\n",
    "epochs = 5001\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(epochs):\n",
    "    scores = score(X_train,Theta)\n",
    "    probabilities = softmax_function(scores)\n",
    "    if i%500 == 0:\n",
    "        print('Iteration #',i,'     Cost function:',cross_entropy_cost_function(y_train_encode,probabilities))\n",
    "    gradient = cross_entropy_gradient(X_train,y_train_encode,probabilities)\n",
    "    Theta = Theta - learning_rate*gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.71815309, -0.26937708, -2.47500322],\n",
       "       [-0.36943744,  0.99537361,  0.74931853],\n",
       "       [-0.79703177, -0.24043082,  1.92060092]])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "def val_score(X_val,y_val,Theta):\n",
    "    scores = score(X_val,Theta)\n",
    "    probabilities = softmax_function(scores)\n",
    "    prediction = np.argmax(probabilities,axis=1)\n",
    "    accuracy = np.mean(prediction == y_val)\n",
    "    print('Accuracy:',accuracy)\n",
    "    return accuracy\n",
    "\n",
    "val_score(X_val,y_val,Theta);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6da2dd8509d2139d132b9c9e65caeab6cdcae61866d81a392c57c7743288e0c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

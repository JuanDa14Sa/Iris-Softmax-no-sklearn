{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,(2,3)]\n",
    "y = iris.target.astype(int)\n",
    "\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 1.4, 0.2],\n",
       "       [1. , 1.4, 0.2],\n",
       "       [1. , 1.3, 0.2],\n",
       "       [1. , 1.5, 0.2],\n",
       "       [1. , 1.4, 0.2]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bias = np.c_[np.ones((len(X),1)),X]\n",
    "num_features = X_bias.shape[1] \n",
    "num_classes = len(np.unique(y))\n",
    "X_bias[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 4.4, 1.3],\n",
       "       [1. , 5.3, 1.9],\n",
       "       [1. , 1.5, 0.2],\n",
       "       [1. , 4.6, 1.4],\n",
       "       [1. , 1.4, 0.2]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test_val_split(X,y,shuffle=True,test_ratio=0.2,val_ratio=0.2):\n",
    "    size = len(X)\n",
    "    if shuffle:\n",
    "        indexes = np.random.permutation(len(X))\n",
    "    else:\n",
    "        indexes = range(len(X))\n",
    "\n",
    "    test_size = int(size*test_ratio)\n",
    "    val_size = int(test_size*val_ratio)\n",
    "    train_size = size-test_size-val_size\n",
    "\n",
    "    X_train = X[indexes[:train_size]]\n",
    "    y_train = y[indexes[:train_size]]\n",
    "    X_val = X[indexes[train_size:-test_size]]\n",
    "    y_val = y[indexes[train_size:-test_size]]\n",
    "    X_test = X[indexes[-test_size:]]\n",
    "    y_test = y[indexes[-test_size:]]\n",
    "\n",
    "    return X_train,y_train,X_val,y_val,X_test,y_test\n",
    "\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = train_test_val_split(X_bias,y)\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hot_encode(y):\n",
    "    num_classes = y.max()+1\n",
    "    m = len(y)\n",
    "    y_hot_encode = np.zeros((m,num_classes))\n",
    "    y_hot_encode[np.arange(m), y] = 1 \n",
    "    return y_hot_encode\n",
    "\n",
    "y_train_encode = hot_encode(y_train)\n",
    "y_test_encode = hot_encode(y_test)\n",
    "y_val_encode = hot_encode(y_val)\n",
    "y_train_encode[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X,Theta):\n",
    "    return X.dot(Theta)\n",
    "\n",
    "def softmax_function(scores):\n",
    "    dividend = np.exp(scores)\n",
    "    divisor = np.sum(dividend, axis=1, keepdims=True)\n",
    "    return dividend/divisor\n",
    "\n",
    "def cross_entropy_cost_function(y, probabilities, Theta=None, regularization=None, alpha=0.5):\n",
    "    cross_entropy = -np.mean(np.sum(y_train_encode*np.log(probabilities),axis=1))\n",
    "    if regularization == None:\n",
    "        return cross_entropy\n",
    "    elif regularization == 'l2':\n",
    "        return cross_entropy+alpha*(1/2)*np.sum(np.square(Theta[1:]))\n",
    "    elif regularization == 'l1':\n",
    "        #in progress\n",
    "        return None\n",
    "    else :\n",
    "        raise Exception('Invalid regularization')\n",
    "           \n",
    "def cross_entropy_gradient(X, y, probabilities, regularization=None,Theta=None,alpha=0.5):\n",
    "    m = len(X)\n",
    "    gradient =  (1/m)*X.T.dot(probabilities-y)\n",
    "    if regularization == None:\n",
    "        return gradient\n",
    "    elif regularization == 'l2':\n",
    "        return gradient+ np.r_[np.zeros([1,num_classes]),Theta[1:]]\n",
    "    elif regularization == 'l1':\n",
    "        #in progress\n",
    "        return None\n",
    "    else :\n",
    "        raise Exception('Invalid regularization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0      Cost function: 1.7809351823164337\n",
      "Iteration # 500      Cost function: 0.7853082500658067\n",
      "Iteration # 1000      Cost function: 0.662175381833871\n",
      "Iteration # 1500      Cost function: 0.5866044593945988\n",
      "Iteration # 2000      Cost function: 0.535453912319336\n",
      "Iteration # 2500      Cost function: 0.49804949859141173\n",
      "Iteration # 3000      Cost function: 0.46909122544766735\n",
      "Iteration # 3500      Cost function: 0.4457124769766781\n",
      "Iteration # 4000      Cost function: 0.4262389179734997\n",
      "Iteration # 4500      Cost function: 0.40962717736569243\n",
      "Iteration # 5000      Cost function: 0.3951912238521942\n"
     ]
    }
   ],
   "source": [
    "Theta_no_reg = np.random.rand(num_features,num_classes)\n",
    "# epsilon = 1e-6\n",
    "epochs = 5001\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(epochs):\n",
    "    scores = score(X_train,Theta_no_reg)\n",
    "    probabilities = softmax_function(scores)\n",
    "    if i%500 == 0:\n",
    "        print('Iteration #',i,'     Cost function:',cross_entropy_cost_function(y_train_encode,probabilities,Theta_no_reg))\n",
    "    gradient = cross_entropy_gradient(X_train,y_train_encode,probabilities)\n",
    "    Theta_no_reg = Theta_no_reg - learning_rate*gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "def val_score(X_val,y_val,Theta):\n",
    "    scores = score(X_val,Theta)\n",
    "    probabilities = softmax_function(scores)\n",
    "    prediction = np.argmax(probabilities,axis=1)\n",
    "    accuracy = np.mean(prediction == y_val)\n",
    "    print('Accuracy:',accuracy)\n",
    "    return accuracy\n",
    "\n",
    "val_score(X_val,y_val,Theta_no_reg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0      Cost function: 2.4131299690876817\n",
      "Iteration # 500      Cost function: 0.7656902144766548\n",
      "Iteration # 1000      Cost function: 0.7636409961963024\n",
      "Iteration # 1500      Cost function: 0.763568139597196\n",
      "Iteration # 2000      Cost function: 0.763564973027806\n",
      "Iteration # 2500      Cost function: 0.763564831941595\n",
      "Iteration # 3000      Cost function: 0.7635648256358217\n",
      "Iteration # 3500      Cost function: 0.7635648253538732\n",
      "Iteration # 4000      Cost function: 0.7635648253412657\n",
      "Iteration # 4500      Cost function: 0.7635648253407021\n",
      "Iteration # 5000      Cost function: 0.7635648253406775\n"
     ]
    }
   ],
   "source": [
    "Theta_l2 = np.random.rand(num_features,num_classes)\n",
    "for i in range(epochs):\n",
    "    scores = score(X_train,Theta_l2)\n",
    "    probabilities = softmax_function(scores)\n",
    "    if i%500 == 0:\n",
    "        print('Iteration #',i,'     Cost function:',cross_entropy_cost_function(y_train_encode,probabilities,Theta_l2,regularization='l2'))\n",
    "    gradient = cross_entropy_gradient(X=X_train,y=y_train_encode,probabilities=probabilities,regularization='l2',Theta=Theta_l2)\n",
    "    Theta_l2 = Theta_l2 - learning_rate*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "val_score(X_val,y_val,Theta_l2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6da2dd8509d2139d132b9c9e65caeab6cdcae61866d81a392c57c7743288e0c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
